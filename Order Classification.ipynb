{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date,timedelta\n",
    "import datetime\n",
    "import math\n",
    "import pyodbc\n",
    "import os, sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "from sqlalchemy import create_engine\n",
    "server = 'bieno-da-d-80166-unilevercom-sql-01.database.windows.net'\n",
    "database = 'bieno-da-d-80166-unilevercom-sqldb-01'\n",
    "username = 'PROD_LoadPool'\n",
    "password = 'Dlk45^8&&dodsslk12Ad7*as'\n",
    "driver = 'SQL Server Native Client 11.0'\n",
    "port = '1433'\n",
    "pd.options.mode.chained_assignment = None\n",
    "data_directory = 'C:/Users/tristan.dresbach/OneDrive - Unilever/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get fullfilled orders:\n",
    "#want to get all orders where quantity expected = quantity dispatched (aka the order was fullfilled in full) for a given date range\n",
    "#the date range is manual as the busisness likes this to be run on an ad hoc basis.\n",
    "sql ='''\n",
    "select MaterialNumber,SalesOrderCode,CustomerSoldTo,ReportingDate, QuantityExpected,QuantityDispatched from RawReporting_BDL.FactSalesOrderLine\n",
    "where ReportingDate >= '2021-03-01' and ReportingDate < '2021-03-24' and QuantityOrdered = QuantityDispatched and QuantityOrdered <> 0 and SalesOrganisationCode in ('0001')\n",
    "'''\n",
    "#EDIT BACK SALES ORGS: '0002','0003','0007'\n",
    "connection_string = 'DRIVER={};SERVER={};PORT={};DATABASE={};UID={};PWD={}'.format(\n",
    "    driver, server, port, database, username, password)\n",
    "cnxn = pyodbc.connect(connection_string)\n",
    "good_orders = pd.read_sql(sql, cnxn)  # Can be assigned to any variable name\n",
    "cnxn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in case you want to run the code for a subset of items, you can import a list of DUs and filter the \"good_orders\" df and the \"orders_cut\" df\n",
    "#that is run below for your selected input items\n",
    "\n",
    "#tgt_items = pd.read_excel('Book1.xlsx')\n",
    "#tgt_items['DU'] = [(18-len(str(x)))*'0' + str(x) for x in tgt_items['DU']]\n",
    "#good_orders = good_orders[good_orders['MaterialNumber'].isin(tgt_items['DU'].tolist())]\n",
    "#good_orders = good_orders.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our cuts column to be used later on\n",
    "\n",
    "good_orders['LBD_QT'] = good_orders['QuantityExpected'] - good_orders['QuantityDispatched']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get cut orders:\n",
    "#the cut column is \"(LI.QuantityLossBeforeDispatch*-1) [LBD_QT]\", we use the same date range as for the previous \"good_orders\" SQL query.\n",
    "# last line of SQL query: \"AND LI.QuantityLossBeforeDispatch <> 0\" guarantees that we are only pulling cuts\n",
    "\n",
    "sql ='''\n",
    "    SELECT\n",
    "            LI.MaterialNumber\n",
    "            , LI.[SalesOrderCode]\n",
    "            , LI.CustomerSoldTo\n",
    "            , LI.ReportingDate\n",
    "            , LI.QuantityExpected\n",
    "            ,LI.QuantityDispatched\n",
    "\n",
    "           \n",
    "           ,(LI.QuantityLossBeforeDispatch*-1) [LBD_QT]\n",
    "           \n",
    "        FROM [RawReporting_BDL].[FactSalesOrderLine] LI\n",
    "        INNER JOIN [TeradataConnect].[DimCalendar] CAL ON LI.ReportingDate = CAL.Date_Dt\n",
    "        INNER JOIN [RawReporting_BDL].[DimSalesOrganisation] AS SO ON LI.SalesOrganisationCode =SO.SalesOrganisationCode\n",
    "        INNER JOIN [RawReporting_BDL].[DimDistributionChannel] AS DC ON LI.DistributionChannelCode =DC.DistributionChannelCode\n",
    "        INNER JOIN [RawReporting_BDL].[HierarchyCordilleraLocalProduct] AS PS ON LI.MaterialNumber =PS.MaterialNumber AND LI.SalesOrganisationCode=PS.SalesOrganisationCode AND LI.DistributionChannelCode=PS.DistributionChannel\n",
    "        WHERE CAL.Days_From_Current > 0 and CAL.Date_Dt >= '2021-03-01' and CAL.Date_dt < '2021-03-24\n",
    "        AND LI.DistributionChannelCode IN ('10','61') ---different distribution channels, \n",
    "        AND LI.SalesOrganisationCode IN ('0001')\n",
    "        AND LI.UnitOfMeasureCode = 'CS'\n",
    "        AND PS.LocalLevel3Code NOT IN ( '006', '0FF', '008', '04D', '0F1')\n",
    "        AND LI.QuantityLossBeforeDispatch <> 0\n",
    "'''\n",
    "#'0002','0003','0007'\n",
    "connection_string = 'DRIVER={};SERVER={};PORT={};DATABASE={};UID={};PWD={}'.format(\n",
    "    driver, server, port, database, username, password)\n",
    "cnxn = pyodbc.connect(connection_string)\n",
    "orders_cut = pd.read_sql(sql, cnxn)  # Can be assigned to any variable name\n",
    "cnxn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these two lines below are part of the same process to filter the data for the items in which we are interested from that input file\n",
    "\n",
    "#orders_cut = orders_cut[orders_cut['MaterialNumber'].isin(tgt_items['DU'].tolist())]\n",
    "#orders_cut = orders_cut.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = pd.concat([good_orders,orders_cut],ignore_index=True)\n",
    "orders.rename(columns={'LBD_QT':'Cuts','ReportingDate':'date'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of sold tos, for names:\n",
    "customers = list(set(orders['CustomerSoldTo']))\n",
    "customers_str = str(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all customer names, using the list of soldtos we just generated from our data:\n",
    "sql ='''\n",
    "select CustomerCode,EccCustomerLevel5Name from RawReporting_BDL.HierarchyCordilleraCustomer\n",
    "where CustomerCode in ''' + customers_str.replace('[','(').replace(']',')')\n",
    "connection_string = 'DRIVER={};SERVER={};PORT={};DATABASE={};UID={};PWD={}'.format(\n",
    "    driver, server, port, database, username, password)\n",
    "cnxn = pyodbc.connect(connection_string)\n",
    "cus_names = pd.read_sql(sql, cnxn)  # Can be assigned to any variable name\n",
    "cnxn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append customer names and merge our base df (\"orders\") with the customer related data:\n",
    "cus_names.rename(columns={'CustomerCode':'CustomerSoldTo','EccCustomerLevel5Name':'Customer'},inplace=True)\n",
    "cus_names = cus_names.drop_duplicates(subset=['CustomerSoldTo'],keep='first').reset_index(drop=True)\n",
    "df = pd.merge(orders,cus_names,how='left',on='CustomerSoldTo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep df for analysis:\n",
    "#we filter out weird data issues like DUs that have empty spaces, weird amount of characters, are = -1\n",
    "#create key column for future looping as well\n",
    "\n",
    "df_f = df.copy()\n",
    "#df_f = df_f.sort_values(['MaterialNumber','date']).reset_index(drop=True)\n",
    "df_f = df_f[~df_f['MaterialNumber'].str.contains(' ')].reset_index(drop=True)\n",
    "df_f = df_f[~df_f['MaterialNumber'].str.len() < 5].reset_index(drop=True)\n",
    "#df_f = df_f[df_f['date'] <= date.today()]\n",
    "#df_f = df_f[(df_f['QuantityDispatched'] >= 0) & (df_f['QuantityOriginal'] > 0)].reset_index(drop=True)\n",
    "df_f = df_f[df_f['MaterialNumber'] != '-1']\n",
    "df_f['key'] = df_f['MaterialNumber'] + '_' + df_f['CustomerSoldTo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in case we want to run our analysis for a specific customer, we can use this filter and input the customer name\n",
    "\n",
    "#df_f = df_f[df_f['Customer'] == 'TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup output:\n",
    "#want to view the data at a DU & lane level hence we do a groupby to know how much should have shipped and wasn't\n",
    "#we create a list of keys from this that will be the DU/lane we are interested in looping through to identify true demand \n",
    "output_df = df_f.groupby(['MaterialNumber','CustomerSoldTo','Customer','key'],as_index=False)[['QuantityExpected','Cuts']].sum()\n",
    "output_df.drop_duplicates(inplace=True)\n",
    "output_df = output_df[output_df['QuantityExpected'] > 1]\n",
    "output_df = output_df.sort_values(['Cuts'],ascending=False)\n",
    "output_df = output_df.reset_index(drop=True)\n",
    "df_f_m = df_f[df_f['key'].isin(list(output_df['key']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup data only for cuts:\n",
    "#make sure that the df we loop through contains the previous list of created unique keys, and then groupby date so that POs\n",
    "#that occured on the same day are see as one smooth PO, helps for the looping\n",
    "study_keys = df_f_m['key'].unique().tolist()\n",
    "study_df = df_f_m[df_f_m['key'].isin(study_keys)]\n",
    "study_df = study_df[['date','QuantityExpected','QuantityDispatched','Cuts','key']]\n",
    "study_df = study_df.groupby(by=['date','key']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes sure for the key in question, we have more than one day of orders. If we only have one day of orders, we can't tell if it is\n",
    "#true demand or not. Create a list of good keys \"keys_goo_len\" to loop through our logic:\n",
    "keys_good_len = []\n",
    "for key in study_keys:\n",
    "    key_df = study_df[study_df['key'] == key]\n",
    "    if len(key_df) > 1:\n",
    "        keys_good_len += [key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000000000067903092_0030003959\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "min() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-613-14ced1c11fbd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mdays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0minit_day\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mdays_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0minit_day\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdays\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: min() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "#initialize variables that we will use to track the output throughout our looping:\n",
    "all_keys = []\n",
    "all_order_qs = []\n",
    "all_dates = []\n",
    "true_demand = []\n",
    "\n",
    "#create function to find if value is in bounds of order quantity and order frequency, identifies which is the issue (could be both):\n",
    "def bounds(q,high_q,low_vol_q,order_indx):\n",
    "    viola = []\n",
    "    if q > high_q:\n",
    "        viola += ['order']\n",
    "    if diff_days[order_indx-1] < low_vol_q:\n",
    "        viola+= ['freq']\n",
    "    return viola\n",
    "\n",
    "\n",
    "#for key in study_keys[0:99]:\n",
    "#for key in keys_good_len[0:99]:\n",
    "for key in ['000000000067903092_0030003959']:\n",
    "    \n",
    "#loop through each key in our good key list:\n",
    "#for key in keys_good_len:\n",
    "    print(key)\n",
    "    #filter df to the key we are looping, get all dates and the key listed the amount of times as the key_df is long to track output\n",
    "    key_df = study_df[study_df['key'] == key]\n",
    "    key_df = key_df.sort_values(by=['date'])\n",
    "    all_dates += key_df['date'].tolist()\n",
    "    all_keys += key_df['key'].tolist()\n",
    "    \n",
    "    #calcualte the difference between days, for order frequency study later\n",
    "    days = sorted(set(key_df['date']))\n",
    "    init_day = min(key_df['date'])\n",
    "    days_list = [(x - init_day).days for x in days]\n",
    "    \n",
    "    #initialize main variables, cuts, orders and dispatched quantity:\n",
    "    cuts_list = []\n",
    "    order_list = []\n",
    "    disp_list = []\n",
    "    for i in range(len(key_df)):\n",
    "        cuts_list += [key_df['Cuts'].tolist()[i]]\n",
    "        order_list += [key_df['QuantityExpected'].tolist()[i]]\n",
    "        disp_list += [key_df['QuantityDispatched'].tolist()[i]]\n",
    "        \n",
    "    #create list of orders that are 100% true:\n",
    "    #we use our true orders as the metrics for what we expect our order quantity and order frequency to be\n",
    "    true_orders = []\n",
    "    n = 0\n",
    "    cut_ct = 0\n",
    "    post_days_counter = 20\n",
    "    true_orders_idx = []\n",
    "    \n",
    "    #calculate what the average, low and high order frequencies are. We keep in cut orders as we want a pessemistic estimate and\n",
    "    #we need enough data points to make this work\n",
    "    diff_days = [y - x for x,y in zip(days_list[:-1],days_list[1:])]\n",
    "    avg_order_days = np.mean(diff_days)\n",
    "    std_order_days = round(np.array(diff_days).std(),2)\n",
    "    std_mult_d = 1.0\n",
    "    high_vol = round(avg_order_days + std_mult_d*std_order_days,1)\n",
    "    low_vol = round(avg_order_days - std_mult_d*std_order_days,1)\n",
    "    if low_vol < 0:\n",
    "        low_vol = 1\n",
    "    \n",
    "    #how far away from a cut order does a fullfilled order have to be to consider that fullfilled order as NOT correlated to the cut:\n",
    "    bound_true_orders  = round(2*avg_order_days,0)\n",
    "    \n",
    "    #identify true orders:\n",
    "    #to identify true orders, we take a look at the below 4 scenarios. We study if it was a cut, and how many days after a cut\n",
    "    #did the order in question we are looking at happen.\n",
    "    while n < len(cuts_list):\n",
    "        #if no cuts and no leading up cuts:       \n",
    "        if ((cuts_list[n] == 0) & (post_days_counter >= bound_true_orders)):\n",
    "            true_orders += [order_list[n]]\n",
    "            true_orders_idx += [n]\n",
    "        #if no leading up cuts and we have cuts:\n",
    "        if ((cuts_list[n] > 0) & (post_days_counter >= bound_true_orders)):\n",
    "            true_orders += [order_list[n]]\n",
    "            post_days_counter = 0\n",
    "            true_orders_idx += [n]\n",
    "        #if we have cuts and there are leading up cuts:    \n",
    "        if ((cuts_list[n]>0) & (post_days_counter < bound_true_orders)):\n",
    "            post_days_counter = 0\n",
    "        #if we have no cuts but leading up cuts:\n",
    "        if ((cuts_list[n] == 0) & (post_days_counter < bound_true_orders)):\n",
    "            if n+1 != len(cuts_list):\n",
    "                day_diff = days_list[n+1] - days_list[n]\n",
    "                post_days_counter += day_diff      \n",
    "        n += 1\n",
    "    \n",
    "    #calculate order metrics:\n",
    "    #need to calculate our low and high bounds for what we expect as a true order:\n",
    "    avg_order_q = np.mean(true_orders)\n",
    "    std_order_q = round(np.array(true_orders).std(),2)\n",
    "    std_mult_q = 2.0\n",
    "    high_order_q = round(avg_order_q + std_mult_q*std_order_q,0)\n",
    "    low_order_q = round(avg_order_q - std_mult_q*std_order_q,0)\n",
    "    if low_order_q < 0:\n",
    "        low_order_q = 1\n",
    "\n",
    "    \n",
    "    true_orders_mod = []\n",
    "    mod_indx = []\n",
    "    for indx in range(len(cuts_list)):\n",
    "        #make sure we are not checking true orders:\n",
    "        if indx not in true_orders_idx:\n",
    "            mod_indx += [indx]\n",
    "            order = order_list[indx]\n",
    "            #we check to see for this not true order, what kind of violations it has (either frequency 'freq' or order quantity related)\n",
    "            violations = bounds(order,high_order_q,low_vol,(indx))\n",
    "            if len(violations)>0:\n",
    "                print(indx,violations)\n",
    "                print(diff_days[indx-1],low_vol,order,high_order_q)\n",
    "                if len(violations) ==1:\n",
    "                    if violations[0] == 'freq':\n",
    "                        #if we have a order frequency violation, we adjust the order quantity down, by the ration between the \n",
    "                        #frequency of this not true demannd order and the low bound of what is acceptable for order frequency\n",
    "                        true_orders_mod += [round(diff_days[indx-1]/low_vol*order,0)]\n",
    "                    else:\n",
    "                        #if not a frequency issue, adjust the order q to the max of our acceptable bounds\n",
    "                        true_orders_mod += [high_order_q]\n",
    "                else:\n",
    "                    #if we are here, then both the freq and order q are an issue, so we adjust for both here:\n",
    "                    true_orders_mod += [round(diff_days[indx-1]/low_vol*high_order_q,0)]\n",
    "            else:\n",
    "                true_orders_mod += [order]\n",
    "    #re-arrange modified order quantities and add them to our tracking lists\n",
    "    for n in range(len(key_df)):\n",
    "            if n in true_orders_idx:\n",
    "                all_order_qs += [true_orders[true_orders_idx.index(n)]]\n",
    "                true_demand += [1]\n",
    "            else:\n",
    "                all_order_qs += [true_orders_mod[mod_indx.index(n)]]\n",
    "                true_demand += [0]\n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create df with output lists from logic:\n",
    "temp = pd.DataFrame()\n",
    "temp['key'] = all_keys\n",
    "temp['Adjusted true demand'] = all_order_qs\n",
    "temp['date'] = all_dates\n",
    "temp['Is QE true demand?'] = true_demand\n",
    "#merge our newly create df with the true demand columns with the columns from the intially generated dfs\n",
    "temp_full = study_df[study_df['key'].isin(temp['key'].tolist())]\n",
    "final_df = pd.merge(temp_full,temp,how='left',on=['key','date'])\n",
    "final_df = final_df.sort_values(by=['key','date']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_adj_cuts = []\n",
    "#need to loop through each unique key in our final df to compensate for change in order quantities\n",
    "#as we have modified order quantirties, the cut quantities and quantity dispatched need to be modified accordingly\n",
    "for key in final_df['key'].unique().tolist():\n",
    "    #create base variables that will be the basis of our logic:\n",
    "    subset_df = final_df[final_df['key'] == key]\n",
    "    QE = subset_df['QuantityExpected'].tolist()\n",
    "    Cuts = subset_df['Cuts'].tolist()\n",
    "    adjD = subset_df['Adjusted true demand'].tolist()\n",
    "    QD = subset_df['QuantityDispatched'].tolist()\n",
    "    running_sum = 0\n",
    "    #loop through each day of orders:\n",
    "    for i in range(len(subset_df)):\n",
    "        #if we have no cuts and quantity expected is the same as adjusted, no modifications\n",
    "        if (Cuts[i] ==0) and (QE[i] == adjD[i]):\n",
    "            all_adj_cuts += [0]\n",
    "        #if we have no cuts and quantity expefcted is different as adjusted quantity, cuts are still 0 but we have \"extra\"\n",
    "        #hypotehtical quantities, so we \"store\" that quantity to decrease later cuts\n",
    "        if (Cuts[i] == 0) and (QE[i] != adjD[i]):\n",
    "            running_sum += QE[i] - adjD[i]\n",
    "            all_adj_cuts += [0]\n",
    "            \n",
    "        if (Cuts[i] != 0):\n",
    "            #if we have cuts, we check to see if we have previously \"stored\" quantities, if not, no problem. If we do,\n",
    "            #we use those stored quantities to decrease cuts (in the first else loop)\n",
    "            if running_sum == 0:\n",
    "                cut_diff = adjD[i] - QD[i]\n",
    "                cut_q_add = Cuts[i] - cut_diff\n",
    "                running_sum += cut_q_add\n",
    "                all_adj_cuts += [cut_diff]\n",
    "            else:\n",
    "                cut_diff = adjD[i] - QD[i]\n",
    "                if running_sum < cut_diff:\n",
    "                    cut_diff -= running_sum\n",
    "                    running_sum = 0\n",
    "                    all_adj_cuts += [cut_diff]\n",
    "                else:\n",
    "                    cut_diff = 0\n",
    "                    running_sum -= cut_diff\n",
    "                    all_adj_cuts += [cut_diff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make cosmetic changes to our final_df, columns requested by the business\n",
    "final_df['Adjusted true cuts'] = all_adj_cuts\n",
    "final_df['Order Q that is over true demand'] = [y-x for x,y in zip(final_df['Adjusted true demand'],final_df['QuantityExpected'])]\n",
    "final_df['Cut Q that is over true cuts'] = [y-x for x,y in zip(final_df['Adjusted true cuts'],final_df['Cuts'])]\n",
    "study = final_df.groupby(['key'],as_index = False).sum()\n",
    "study['DU'] = [x.split('_')[0] for x in study['key']]\n",
    "study = study.groupby(['DU'],as_index=False).sum()\n",
    "study = study.sort_values(by=['Order Q that is over true demand'],ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add in description, UPC and GTIN:\n",
    "sql ='''\n",
    "\n",
    "select MaterialNumber as DU, EANOrUPC,MaterialDescription,MaterialGtin from [RawReporting_BDL].DimMaterial '''\n",
    "#EDIT BACK SALES ORGS: '0002','0003','0007'\n",
    "connection_string = 'DRIVER={};SERVER={};PORT={};DATABASE={};UID={};PWD={}'.format(\n",
    "    driver, server, port, database, username, password)\n",
    "cnxn = pyodbc.connect(connection_string)\n",
    "join_data = pd.read_sql(sql, cnxn)  # Can be assigned to any variable name\n",
    "cnxn.close()\n",
    "\n",
    "full_study = pd.merge(study,join_data,how='left',on='DU').reset_index()\n",
    "full_study['percentage of total'] = [  round((100*x/sum(full_study['Order Q that is over true demand'])),2)  for x in full_study['Order Q that is over true demand']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export to excel for business use:\n",
    "#full_study.to_excel('study_tgt_all_items_c.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
